{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SlayerDraco/pr0mp7_3nh4nc3r/blob/main/pr0mp7_3nh4nc3r.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# New Cell: Installation and Setup\n",
        "!pip install -q google-generativeai\n",
        "\n",
        "import google.generativeai as genai\n",
        "from google.colab import userdata\n",
        "\n",
        "# Configure the API client with your secret key\n",
        "try:\n",
        "    genai.configure(api_key=userdata.get('GEMINI_API_KEY'))\n",
        "    # Initialize the model\n",
        "    llm = genai.GenerativeModel('models/gemini-flash-latest')\n",
        "    print(\"Gemini model configured successfully.\")\n",
        "except Exception as e:\n",
        "    llm = None\n",
        "    print(f\"Error configuring Gemini model: {e}\")"
      ],
      "metadata": {
        "id": "NCenRj_hqPwX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install symspellpy\n",
        "!pip install textblob\n",
        "!pip install scikit-learn pandas joblib\n",
        "!python -m textblob.download_corpora"
      ],
      "metadata": {
        "collapsed": true,
        "id": "70VGAPIN_6Vc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MY5kCD271iO9"
      },
      "outputs": [],
      "source": [
        "from symspellpy.symspellpy import SymSpell, Verbosity\n",
        "from textblob import TextBlob\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "from datetime import datetime\n",
        "import os\n",
        "import joblib\n",
        "import random\n",
        "import pandas as pd\n",
        "import csv\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- ML Model Training Cell ---\n",
        "# This cell trains the model only if the model file doesn't exist.\n",
        "\n",
        "model_filename = 'intent_model.joblib'\n",
        "data_filename = 'training_dataset.csv'\n",
        "\n",
        "if not os.path.exists(model_filename):\n",
        "    print(f\"[*] Model file '{model_filename}' not found. Training a new model...\")\n",
        "    try:\n",
        "        # 1. Load Data\n",
        "        df = pd.read_csv(data_filename)\n",
        "\n",
        "        # 2. Define Features (X) and Target (y)\n",
        "        X = df['prompt']\n",
        "        y = df['intent']\n",
        "\n",
        "        # 3. Create and Train the ML Pipeline\n",
        "        pipeline = Pipeline([\n",
        "            ('tfidf', TfidfVectorizer(stop_words='english', ngram_range=(1,2))),\n",
        "            ('clf', LogisticRegression(random_state=42)),\n",
        "        ])\n",
        "        pipeline.fit(X, y)\n",
        "\n",
        "        # 4. Save the trained pipeline\n",
        "        joblib.dump(pipeline, model_filename)\n",
        "        print(f\"[+] Model trained and saved to '{model_filename}'\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"[!] Error: Training data '{data_filename}' not found. Please create or upload it.\")\n",
        "        intent_model = None\n",
        "else:\n",
        "    print(f\"[*] Model file '{model_filename}' found. Skipping training.\")\n",
        "\n",
        "# --- Load the Model ---\n",
        "try:\n",
        "    intent_model = joblib.load(model_filename)\n",
        "    print(\"[+] Intent model loaded successfully.\")\n",
        "except FileNotFoundError:\n",
        "    print(\"[!] Failed to load the model.\")\n",
        "    intent_model = None"
      ],
      "metadata": {
        "id": "kTumcaB4LWGk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dictionary_path = \"frequency_dictionary_en_82_765.txt\"\n",
        "max_edit_distance_dictionary = 2\n",
        "prefix_length = 7\n",
        "\n",
        "sym_spell = SymSpell(max_edit_distance_dictionary, prefix_length)\n",
        "if not sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1):\n",
        "    print(f\"[!] Failed to load dictionary from {dictionary_path}\")\n",
        "\n",
        "# Custom technical/AI words to preserve\n",
        "custom_words = [\n",
        "    \"python\", \"javascript\", \"html\", \"react\", \"robot\",\n",
        "    \"dataset\", \"API\", \"GPT\", \"AI\", \"ML\", \"cybersecurity\",\n",
        "    \"blockchain\", \"deepfake\", \"docker\", \"kubernetes\", \"quiksort\",\n",
        "    \"matlab\", \"waf\", \"sql\", \"geolocation\", \"osint\", \"payload\",\n",
        "    \"BCC\", \"FCC\"\n",
        "]"
      ],
      "metadata": {
        "id": "dqKIBP-F2n59"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def correct_with_symspell(text: str) -> str:\n",
        "    corrected_words = []\n",
        "    for word in text.split():\n",
        "        # Preserve custom words (technical, slang, names)\n",
        "        if word.lower() in [w.lower() for w in custom_words]:\n",
        "            corrected_words.append(word)\n",
        "            continue\n",
        "\n",
        "        # Skip very short words (1-2 letters), likely abbreviations/slang\n",
        "        if len(word) <= 2:\n",
        "            corrected_words.append(word)\n",
        "            continue\n",
        "\n",
        "        # Get SymSpell suggestions\n",
        "        suggestions = sym_spell.lookup(word, Verbosity.CLOSEST, max_edit_distance=2)\n",
        "\n",
        "        if suggestions:\n",
        "            # Only accept suggestion if it's a real improvement (edit distance > 0)\n",
        "            best_suggestion = suggestions[0].term\n",
        "            if best_suggestion.lower() != word.lower():\n",
        "                corrected_words.append(best_suggestion)\n",
        "            else:\n",
        "                corrected_words.append(word)\n",
        "        else:\n",
        "            # No suggestions, keep original word\n",
        "            corrected_words.append(word)\n",
        "\n",
        "    return \" \".join(corrected_words)"
      ],
      "metadata": {
        "id": "pVwJBHDVBUwU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_intent_hybrid(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Detects intent using a hybrid approach:\n",
        "    1. Get a prediction from the ML model.\n",
        "    2. Use rule-based keywords to override and correct the prediction if necessary.\n",
        "    \"\"\"\n",
        "    # Step 1: Get the baseline prediction from the ML model\n",
        "    if not intent_model:\n",
        "        return \"GENERAL_QUERY\" # Fallback\n",
        "\n",
        "    predicted_intent = intent_model.predict([text])[0]\n",
        "\n",
        "    text_lower = text.lower()\n",
        "    # Rule for PROJECT_BRIEF\n",
        "    project_keywords = ['build an interactive', 'design a tool', 'create a concept', 'design a dashboard', 'plan a database']\n",
        "    if any(keyword in text_lower for keyword in project_keywords):\n",
        "        return \"PROJECT_BRIEF\"\n",
        "\n",
        "    # Rule for TECHNICAL_EXPLANATION\n",
        "    explanation_keywords = ['how to bypass', 'explain the concept', 'what is the difference','function to solve']\n",
        "    if any(keyword in text_lower for keyword in explanation_keywords):\n",
        "        return \"TECHNICAL_EXPLANATION\"\n",
        "\n",
        "    # Step 3: If no specific rules are met, trust the original ML prediction.\n",
        "    return predicted_intent\n"
      ],
      "metadata": {
        "id": "PzLxIvbt2qsF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Updated Function: Replaces the previous enhance_with_llm\n",
        "\n",
        "def enhance_with_llm(intent: str, prompt: str) -> str:\n",
        "    \"\"\"\n",
        "    Uses an LLM (Gemini) to perform the final, knowledge-driven enhancement,\n",
        "    with constraints on output length.\n",
        "    \"\"\"\n",
        "    if not llm:\n",
        "        return \"Error: LLM not configured. Cannot perform enhancement.\"\n",
        "\n",
        "    # A dictionary of expert personas and instructions for the LLM.\n",
        "    # Now updated with length constraints.\n",
        "    meta_prompts = {\n",
        "        \"PROJECT_BRIEF\": \"\"\"\n",
        "            You are an expert project manager. A user will provide a vague project idea.\n",
        "            Rewrite it into a detailed but concise project specification, NOT TO EXCEED 8 LINES.\n",
        "            The specification must define:\n",
        "            1. The core product and its target audience.\n",
        "            2. 2-3 essential key features or user stories.\n",
        "            3. The desired tone or style (e.g., professional, modern, minimalist).\n",
        "        \"\"\",\n",
        "        \"TECHNICAL_EXPLANATION\": \"\"\"\n",
        "            You are an expert technical writer. A user will ask a vague technical question.\n",
        "            Rewrite it into a detailed but concise prompt for an expert system, NOT TO EXCEED 8 LINES.\n",
        "            The prompt must clearly state:\n",
        "            1. The core topic.\n",
        "            2. 3-4 specific sub-topics or concepts that must be covered in the answer.\n",
        "            3. The intended audience (e.g., beginner, university student, professional).\n",
        "        \"\"\",\n",
        "        \"IMAGE_CREATION\": \"\"\"\n",
        "            You are a professional art director. A user will describe an image.\n",
        "            Rewrite it into a hyper-detailed but concise prompt for an image generation AI, NOT TO EXCEED 8 LINES.\n",
        "            The prompt must specify:\n",
        "            1. The primary subject and composition.\n",
        "            2. The artistic style (e.g., photorealism, digital painting, anime).\n",
        "            3. Specific details about lighting, camera angle, and color palette.\n",
        "        \"\"\",\n",
        "        \"CODE_GENERATION\": \"\"\"\n",
        "            You are an expert software architect. A user will ask for a script.\n",
        "            Rewrite their request into a clear and concise code specification for a senior developer, NOT TO EXCEED 8 LINES.\n",
        "            The prompt must specify:\n",
        "            1. The core function of the script.\n",
        "            2. The required inputs and expected outputs.\n",
        "            3. Key requirements like error handling, comments, or specific libraries to use.\n",
        "        \"\"\",\n",
        "        \"CREATIVE_WRITING\": \"\"\"\n",
        "            You are a seasoned story editor. A user will provide a story idea.\n",
        "            Rewrite it into a compelling story brief for a professional author, NOT TO EXCEED 8 LINES.\n",
        "            The brief must establish:\n",
        "            1. The genre and setting (e.g., cyberpunk, fantasy).\n",
        "            2. The protagonist and their core conflict or goal.\n",
        "            3. The central theme and desired tone of the story.\n",
        "        \"\"\"\n",
        "    }\n",
        "\n",
        "    instructions = meta_prompts.get(intent, \"You are a helpful assistant. Rewrite the following user prompt to be more clear and detailed, keeping it under 8 lines.\")\n",
        "\n",
        "    final_prompt_for_llm = f\"{instructions}\\n\\nUser's Prompt: '{prompt}'\"\n",
        "\n",
        "    try:\n",
        "        response = llm.generate_content(final_prompt_for_llm)\n",
        "        return response.text.strip()\n",
        "    except Exception as e:\n",
        "        return f\"Error during LLM enhancement: {e}\""
      ],
      "metadata": {
        "id": "JT8-_Js02r7F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Update the main pipeline function\n",
        "\n",
        "def enhance_prompt(broken_prompt: str, show_corrections: bool = True) -> str:\n",
        "    \"\"\"\n",
        "    The complete, final pipeline.\n",
        "    \"\"\"\n",
        "    print(f\"\\n--- Enhancing Prompt ---\")\n",
        "\n",
        "    corrected = correct_with_symspell(broken_prompt)\n",
        "    intent = get_intent_hybrid(corrected)\n",
        "\n",
        "    # --- HERE IS THE SWAP ---\n",
        "    # Replace the old keyword expander with the new LLM enhancer\n",
        "    final_enhanced_prompt = enhance_with_llm(intent, corrected)\n",
        "\n",
        "    if show_corrections:\n",
        "        print(f\"[Original]   {broken_prompt}\")\n",
        "        print(f\"[Corrected]  {corrected}\")\n",
        "        print(f\"[Intent]     {intent} (Predicted by Hybrid Model)\")\n",
        "        print(\"\\n--- Final LLM-Enhanced Prompt ---\")\n",
        "\n",
        "    return final_enhanced_prompt"
      ],
      "metadata": {
        "id": "2VCh5SM_2v_8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def log_feedback(prompt, corrected_prompt, intent, enhanced_prompt, was_correct, corrected_intent=None):\n",
        "    \"\"\"\n",
        "    Logs the enhancer's performance and user feedback to a CSV file.\n",
        "    \"\"\"\n",
        "    log_file = 'feedback_log.csv'\n",
        "    file_exists = os.path.exists(log_file)\n",
        "\n",
        "    fieldnames = [\n",
        "        'timestamp', 'original_prompt', 'corrected_prompt',\n",
        "        'predicted_intent', 'enhanced_prompt', 'was_correct', 'user_corrected_intent'\n",
        "    ]\n",
        "\n",
        "    with open(log_file, 'a', newline='', encoding='utf-8') as f:\n",
        "        writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
        "        if not file_exists:\n",
        "            writer.writeheader()  # Write headers only if the file is new\n",
        "\n",
        "        writer.writerow({\n",
        "            'timestamp': datetime.now().isoformat(),\n",
        "            'original_prompt': prompt,\n",
        "            'corrected_prompt': corrected_prompt,\n",
        "            'predicted_intent': intent,\n",
        "            'enhanced_prompt': enhanced_prompt,\n",
        "            'was_correct': was_correct,\n",
        "            'user_corrected_intent': corrected_intent if corrected_intent else 'N/A'\n",
        "        })"
      ],
      "metadata": {
        "id": "IdSkYNEeaYHc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# New Final Cell: Main Interactive Loop\n",
        "\n",
        "print(\"--- Prompt Enhancer v3.0 ---\")\n",
        "print(\"Type your prompt below, or type 'quit/q' to exit.\")\n",
        "\n",
        "# We need to get the intent out of the enhance_prompt function for logging,\n",
        "# so we'll modify it slightly to return both the prompt and the intent.\n",
        "def enhance_prompt_with_intent(broken_prompt: str):\n",
        "    corrected = correct_with_symspell(broken_prompt)\n",
        "    intent = get_intent_hybrid(corrected)\n",
        "    final_enhanced_prompt = enhance_with_llm(intent, corrected)\n",
        "    return final_enhanced_prompt, intent, corrected\n",
        "\n",
        "# This is the main loop\n",
        "while True:\n",
        "    print(\"-\" * 30)\n",
        "    user_prompt = input(\"Your Prompt: \")\n",
        "\n",
        "    if user_prompt.lower() in ['quit', 'exit', 'q']:\n",
        "        print(\"Exiting session. Goodbye!\")\n",
        "        break\n",
        "\n",
        "    # Get the enhanced prompt and the data for logging\n",
        "    enhanced_prompt, predicted_intent, corrected_prompt = enhance_prompt_with_intent(user_prompt)\n",
        "\n",
        "    print(\"\\n--- LLM-Enhanced Prompt ---\")\n",
        "    print(enhanced_prompt)\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "    # --- The Feedback Loop ---\n",
        "    feedback = ''\n",
        "    while feedback not in ['y', 'n']:\n",
        "        feedback = input(\"Was this enhancement good? (y/n): \").lower()\n",
        "\n",
        "    if feedback == 'y':\n",
        "        # \"Appreciate\": Log the success\n",
        "        log_feedback(user_prompt, corrected_prompt, predicted_intent, enhanced_prompt, was_correct=True)\n",
        "        print(\"[+] Success logged. Thank you!\")\n",
        "    else:\n",
        "        # \"Punish\": Log the failure and ask for the correct intent\n",
        "        print(\"Sorry about that. To help the model learn, please provide the correct intent.\")\n",
        "        print(\"Options: [CODE_GENERATION, IMAGE_CREATION, PROJECT_BRIEF, TECHNICAL_EXPLANATION, CREATIVE_WRITING, GENERAL_QUERY]\")\n",
        "        correct_intent = input(\"What should the intent have been? \")\n",
        "        log_feedback(user_prompt, corrected_prompt, predicted_intent, enhanced_prompt, was_correct=False, corrected_intent=correct_intent)\n",
        "        print(\"[!] Failure and correction logged. This will be used for future training.\")"
      ],
      "metadata": {
        "id": "_1YBIz8nb2GP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}